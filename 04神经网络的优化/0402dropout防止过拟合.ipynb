{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Iter: 0, Testing accuracy:0.920000----training accuracy:0.913236\n",
      "Iter: 1, Testing accuracy:0.929100----training accuracy:0.927727\n",
      "Iter: 2, Testing accuracy:0.938200----training accuracy:0.935073\n",
      "Iter: 3, Testing accuracy:0.939400----training accuracy:0.940145\n",
      "Iter: 4, Testing accuracy:0.942400----training accuracy:0.944473\n",
      "Iter: 5, Testing accuracy:0.948000----training accuracy:0.949764\n",
      "Iter: 6, Testing accuracy:0.950000----training accuracy:0.951745\n",
      "Iter: 7, Testing accuracy:0.950400----training accuracy:0.954109\n",
      "Iter: 8, Testing accuracy:0.954900----training accuracy:0.956400\n",
      "Iter: 9, Testing accuracy:0.955600----training accuracy:0.958855\n",
      "Iter: 10, Testing accuracy:0.957200----training accuracy:0.960855\n",
      "Iter: 11, Testing accuracy:0.956600----training accuracy:0.961873\n",
      "Iter: 12, Testing accuracy:0.956800----training accuracy:0.962218\n",
      "Iter: 13, Testing accuracy:0.959600----training accuracy:0.964545\n",
      "Iter: 14, Testing accuracy:0.961100----training accuracy:0.965473\n",
      "Iter: 15, Testing accuracy:0.960600----training accuracy:0.966545\n",
      "Iter: 16, Testing accuracy:0.962500----training accuracy:0.968073\n",
      "Iter: 17, Testing accuracy:0.962500----training accuracy:0.969018\n",
      "Iter: 18, Testing accuracy:0.963800----training accuracy:0.969800\n",
      "Iter: 19, Testing accuracy:0.965200----training accuracy:0.971000\n",
      "Iter: 20, Testing accuracy:0.965900----training accuracy:0.971491\n",
      "Iter: 21, Testing accuracy:0.966800----training accuracy:0.972364\n",
      "Iter: 22, Testing accuracy:0.966500----training accuracy:0.972364\n",
      "Iter: 23, Testing accuracy:0.968000----training accuracy:0.973473\n",
      "Iter: 24, Testing accuracy:0.968100----training accuracy:0.974345\n",
      "Iter: 25, Testing accuracy:0.968900----training accuracy:0.974982\n",
      "Iter: 26, Testing accuracy:0.969200----training accuracy:0.975055\n",
      "Iter: 27, Testing accuracy:0.969400----training accuracy:0.975818\n",
      "Iter: 28, Testing accuracy:0.971000----training accuracy:0.976491\n",
      "Iter: 29, Testing accuracy:0.971600----training accuracy:0.976818\n",
      "Iter: 30, Testing accuracy:0.971200----training accuracy:0.977782\n"
     ]
    }
   ],
   "source": [
    "#数据载入---有其他方法\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True) #注意输出标签是one_hot类型\n",
    "\n",
    "#按批次进行计算\n",
    "#设置每个批次大小\n",
    "batch_size = 100\n",
    "#获取批次数量，注意：我们训练时使用的是mnist.train训练集数据\n",
    "n_batch = mnist.train.num_examples // batch_size #注意：//也是除法，返回整数\n",
    "\n",
    "#开始定义占位符号\n",
    "x = tf.placeholder(tf.float32,[None,784]) #由于每张图片都是28×28像素，所以每个样本的特征都是784\n",
    "y = tf.placeholder(tf.float32,[None,10]) #是分10类\n",
    "keep_prob = tf.placeholder(tf.float32) #设置全局dropout参数,即设置每个神经元正常工作的概率\n",
    "\n",
    "#开始创建一个神经网络隐藏层L1含有10个激活单元\n",
    "Weight_L1 = tf.Variable(tf.truncated_normal([784,2000],stddev=0.1)) #产生截断正态分布随机数，取值范围为[ mean - 2 * stddev, mean + 2 * stddev ]  截断的产生正态分布的随机数，即随机数与均值的差值若大于两倍的标准差，则重新生成\n",
    "biases_L1 = tf.Variable(tf.zeros([1,2000])+0.1) #对于偏执单元的权重，我们没有必要进行随机初始化,但是可以设置初始值\n",
    "Wx_plus_b_L1 = tf.matmul(x,Weight_L1)+biases_L1\n",
    "L1 = tf.nn.tanh(Wx_plus_b_L1)\n",
    "L1_drop = tf.nn.dropout(L1,keep_prob) #设置该L1层的dropout信息\n",
    "\n",
    "Weight_L2 = tf.Variable(tf.truncated_normal([2000,2000],stddev=0.1)) #产生截断正态分布随机数，取值范围为[ mean - 2 * stddev, mean + 2 * stddev ]  截断的产生正态分布的随机数，即随机数与均值的差值若大于两倍的标准差，则重新生成\n",
    "biases_L2 = tf.Variable(tf.zeros([1,2000])+0.1) #对于偏执单元的权重，我们没有必要进行随机初始化,但是可以设置初始值\n",
    "Wx_plus_b_L2 = tf.matmul(L1_drop,Weight_L2)+biases_L2\n",
    "L2 = tf.nn.tanh(Wx_plus_b_L2)\n",
    "L2_drop = tf.nn.dropout(L2,keep_prob) #设置该L1层的dropout信息\n",
    "\n",
    "Weight_L3 = tf.Variable(tf.truncated_normal([2000,1000],stddev=0.1)) #产生截断正态分布随机数，取值范围为[ mean - 2 * stddev, mean + 2 * stddev ]  截断的产生正态分布的随机数，即随机数与均值的差值若大于两倍的标准差，则重新生成\n",
    "biases_L3 = tf.Variable(tf.zeros([1,1000])+0.1) #对于偏执单元的权重，我们没有必要进行随机初始化,但是可以设置初始值\n",
    "Wx_plus_b_L3 = tf.matmul(L2_drop,Weight_L3)+biases_L3\n",
    "L3 = tf.nn.tanh(Wx_plus_b_L3)\n",
    "L3_drop = tf.nn.dropout(L3,keep_prob) #设置该L1层的dropout信息\n",
    "\n",
    "Weight_L4 = tf.Variable(tf.truncated_normal([1000,10],stddev=0.1)) #产生截断正态分布随机数，取值范围为[ mean - 2 * stddev, mean + 2 * stddev ]  截断的产生正态分布的随机数，即随机数与均值的差值若大于两倍的标准差，则重新生成\n",
    "biases_L4 = tf.Variable(tf.zeros([1,10])+0.1) #对于偏执单元的权重，我们没有必要进行随机初始化,但是可以设置初始值\n",
    "Wx_plus_b_L4 = tf.matmul(L3_drop,Weight_L4)+biases_L4\n",
    "y_pred = tf.nn.softmax(Wx_plus_b_L4)\n",
    "\n",
    "#定义代价函数\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=y_pred)) #这里我们使用交叉熵进行处理，使得加快训练速度\n",
    "#使用梯度下降法\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#获取结果存放在bool列表中\n",
    "correct_prediction = tf.equal(tf.arg_max(y,1),tf.arg_max(y_pred,1)) #注意：arg_max第二个参数是指定坐标信息，因为结果按行存储，所以1\n",
    "#根据上面的bool列表，获取准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)) #tf.cast格式转换后，求解平均值\n",
    "\n",
    "\n",
    "#开启会话\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for iter_cnt in range(31): #整体数据迭代\n",
    "        for batch_cnt in range(n_batch): #小批次数据分批次迭代\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size) #依照batch_size获取下一批次数据\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:0.7}) #传入数据，进行训练。对于隐藏层每一层随机使用70%的激活单元正常工作，另外30%不工作防止过拟合\n",
    "            \n",
    "        #获取每一次整体数据迭代后的准确率\n",
    "        #在我们进行训练权值时，我们需要设置dropout参数去降低拟合程度，我们训练完毕以后，需要我们预测时，是不需要进行甚至dropout参数的，所以下面我们都是设置为1\n",
    "        test_acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0}) #注意：预测时，我们也要传入数据，并且传入的是测试集数据\n",
    "        train_acc = sess.run(accuracy,feed_dict={x:mnist.train.images,y:mnist.train.labels,keep_prob:1.0}) #注意：我们需要测试集与训练集对比，看是否过拟合\n",
    "        print(\"Iter: %d, Testing accuracy:%f----training accuracy:%f\"%(iter_cnt,test_acc,train_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep_drop=1.0,即不使用dropout时\n",
    "Iter: 29, Testing accuracy:0.970800----training accuracy:0.995291\n",
    "Iter: 30, Testing accuracy:0.970800----training accuracy:0.995345\n",
    "            \n",
    "#keep_drop=0.7,使用dropout,可以发现我们训练集预测和测试集预测相差不大，想比较keep_drop=1时，没有明显过拟合现象\n",
    "Iter: 29, Testing accuracy:0.971600----training accuracy:0.976818\n",
    "Iter: 30, Testing accuracy:0.971200----training accuracy:0.977782"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
