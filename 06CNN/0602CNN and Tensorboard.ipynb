{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://www.ai-start.com/dl2017/html/lesson4-week1.html\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-5ce0dc96c1d3>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/ld/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/ld/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ld/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ld/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ld/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-2-5ce0dc96c1d3>:89: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-2-5ce0dc96c1d3>:102: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Iter:0, Test accuracy:0.100400 Train accuracy:0.094000\n",
      "Iter:100, Test accuracy:0.618600 Train accuracy:0.603800\n",
      "Iter:200, Test accuracy:0.851400 Train accuracy:0.844100\n",
      "Iter:300, Test accuracy:0.920000 Train accuracy:0.917400\n",
      "Iter:400, Test accuracy:0.932800 Train accuracy:0.930600\n",
      "Iter:500, Test accuracy:0.943000 Train accuracy:0.940200\n",
      "Iter:600, Test accuracy:0.949000 Train accuracy:0.950500\n",
      "Iter:700, Test accuracy:0.953800 Train accuracy:0.954200\n",
      "Iter:800, Test accuracy:0.953000 Train accuracy:0.956700\n",
      "Iter:900, Test accuracy:0.960000 Train accuracy:0.959400\n",
      "Iter:1000, Test accuracy:0.962800 Train accuracy:0.963000\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "#初始化权值\n",
    "def weight_variable(shape):\n",
    "    init = tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "#初始化偏执单元对应权值\n",
    "def bias_variable(shape):\n",
    "    init = tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "#实现卷积层\n",
    "def conv2d(x,W):\n",
    "    #input : 输入的要做卷积的图片，要求为一个张量，shape为 [ batch, in_height, in_weight, in_channel ]，其中batch为图片的数量，in_height 为图片高度，in_weight 为图片宽度，in_channel 为图片的通道数，灰度图该值为1，彩色图为3。（也可以用其它值，但是具体含义不是很理解）\n",
    "    #filter： 卷积核，要求也是一个张量，shape为 [ filter_height, filter_weight, in_channel, out_channels ]，其中 filter_height 为卷积核高度，filter_weight 为卷积核宽度，in_channel 是图像通道数 ，和 input 的 in_channel 要保持一致，out_channel 是卷积核数量。\n",
    "    #strides： 卷积时在图像每一维的步长，这是一个一维的向量，[ 1, strides, strides, 1]，第一位和最后一位固定必须是1\n",
    "    #padding： string类型，值为“SAME” 和 “VALID”，表示的是卷积的形式，是否考虑边界。\"SAME\"是考虑边界，不足的时候用0去填充周围，\"VALID\"则不考虑\n",
    "    #use_cudnn_on_gpu： bool类型，是否使用cudnn加速，默认为true\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding=\"SAME\")\n",
    "\n",
    "#实现池化层\n",
    "def max_pool_2x2(x):\n",
    "    #value : 需要池化的输入，一般池化层接在卷积层后面，所以输入通常是feature map，依然是[batch_size, height, width, channels]这样的shape\n",
    "    #k_size : 池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]，因为我们不想在batch和channels上做池化，所以这两个维度设为了1\n",
    "    #strides : 窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]\n",
    "    #padding： 填充的方法，SAME或VALID\n",
    "    return tf.nn.max_pool(x,[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "with tf.name_scope(\"input\"):\n",
    "    #定义两个输入占位符号\n",
    "    x = tf.placeholder(tf.float32,[None,784],name=\"x-input\")\n",
    "    y = tf.placeholder(tf.float32,[None,10],name=\"y-input\")\n",
    "    with tf.name_scope(\"x_image\"):\n",
    "        #改变x的格式转化为4D的响亮[batch,in_height,in_width,in_channels]\n",
    "        x_image = tf.reshape(x,[-1,28,28,1]) #batch：-1 的应用:-1 表示不知道该填什么数字合适的情况下，可以选择，由python通过a和其他的值3推测出来\n",
    "\n",
    "with tf.name_scope(\"Conv1\"):        \n",
    "    with tf.name_scope(\"W_conv1\"):\n",
    "        #初始化的一个卷基层的权值和偏执\n",
    "        W_conv1 = weight_variable([5,5,1,32]) #5*5的采样窗口，1表示输入通道为1,黑白。32表示输出的通道数--表示使用了32个卷积核（最终会获得32个特征平面）\n",
    "    with tf.name_scope(\"b_conv1\"):\n",
    "        b_conv1 = bias_variable([32]) #每一个卷积核都需要一个偏执，所以我们这里要32个偏置值\n",
    "\n",
    "    with tf.name_scope(\"relu\"):        \n",
    "        #获取第一层卷积之后的激活值，以及池化层处理以后的隐藏层信息\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1)\n",
    "    with tf.name_scope(\"h_pool1\"):\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "with tf.name_scope(\"Conv2\"):        \n",
    "    with tf.name_scope(\"W_conv2\"):\n",
    "        #初始化第二个卷积层的权值和偏置\n",
    "        W_conv2 = weight_variable([5,5,32,64]) #对比上一个的卷积层权值初始化，这里采用5*5采样，32表示输入的维度，64表示使用的卷积核数量（输出的维度）\n",
    "    with tf.name_scope(\"b_conv2\"):\n",
    "        b_conv2 = bias_variable([64])\n",
    "\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        #二次卷积、池化操作\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2)\n",
    "    with tf.name_scope(\"h_pool2\"):\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "#总结上面两次卷积池化操作\n",
    "#第一次：卷积后28×28不变，池化后14×14\n",
    "#第二次：卷积后14×14不变，池化后7×7\n",
    "#获取了64个7×7二维数据(7*7*64个神经单元)\n",
    "\n",
    "    with tf.name_scope(\"h_pool2_flat\"):\n",
    "        #先将第二次池化层处理后的数据扁平化\n",
    "        h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])\n",
    "\n",
    "with tf.name_scope(\"fc1\"):        \n",
    "    with tf.name_scope(\"W_fc1\"):\n",
    "        #初始化第一个全链接层权值\n",
    "        W_fc1 = weight_variable([7*7*64,1024]) #设置第一个全链接层单元数量为1024\n",
    "    with tf.name_scope(\"b_fc1\"):\n",
    "        b_fc1 = bias_variable([1024]) #共有1024个目标单元\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        #求得第一个全链接层的输出\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1)\n",
    "    with tf.name_scope(\"keep_drop\"):\n",
    "        #设置dropout信息\n",
    "        keep_drop = tf.placeholder(tf.float32)\n",
    "    with tf.name_scope(\"h_fc1_drop\"):\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1,keep_drop)\n",
    "\n",
    "with tf.name_scope(\"fc2\"):\n",
    "    with tf.name_scope(\"W_fc2\"):\n",
    "        #初始化第二个全链接层权值\n",
    "        W_fc2 = weight_variable([1024,10]) #开始要进行输出了\n",
    "    with tf.name_scope(\"b_fc2\"):\n",
    "        b_fc2 = bias_variable([10])\n",
    "    with tf.name_scope(\"softmax\"):\n",
    "        prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    #交叉熵处理获取损失\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n",
    "    tf.summary.scalar(\"loss\",loss)\n",
    "\n",
    "with tf.name_scope(\"train_step\"):\n",
    "    #根据损失函数进行优化\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    \n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    with tf.name_scope(\"correct_prediction\"):\n",
    "        #返回预测结果到bool列表\n",
    "        corrent_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "    with tf.name_scope(\"acc\"):\n",
    "        #返回准确率tf.cast强制转换\n",
    "        accuracy = tf.reduce_mean(tf.cast(corrent_prediction,tf.float32)) \n",
    "        tf.summary.scalar(\"accuracy\",accuracy)\n",
    "        \n",
    "merged = tf.summary.merge_all(); #主要就是上面的预测进行了merged操作，所以下面我们直接使用run(merged...)就是进行预测操作\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter(\"logs/train\",sess.graph) #存入两个不同文件\n",
    "    test_writer = tf.summary.FileWriter(\"logs/test\",sess.graph)\n",
    "    for iter in range(1001):\n",
    "        batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "        sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_drop:0.5})\n",
    "        #记录训练集计算的参数\n",
    "        summary = sess.run(merged,feed_dict={x:batch_xs,y:batch_ys,keep_drop:1.0})\n",
    "        train_writer.add_summary(summary,iter)\n",
    "        #记录测试集计算的参数\n",
    "        batch_xs,batch_ys = mnist.test.next_batch(batch_size)\n",
    "        summary = sess.run(merged,feed_dict={x:batch_xs,y:batch_ys,keep_drop:1.0})\n",
    "        test_writer.add_summary(summary,iter)\n",
    "            \n",
    "        if iter%100 == 0:\n",
    "            test_acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_drop:1.0})\n",
    "            train_acc = sess.run(accuracy,feed_dict={x:mnist.train.images[:10000],y:mnist.train.labels[:10000],keep_drop:1.0})\n",
    "            print(\"Iter:%d, Test accuracy:%f Train accuracy:%f\"%(iter,test_acc,train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
