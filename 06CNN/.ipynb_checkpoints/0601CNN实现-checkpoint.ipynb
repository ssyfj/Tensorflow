{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://www.ai-start.com/dl2017/html/lesson4-week1.html\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Iter:0, Test accuracy:0.865700\n",
      "Iter:1, Test accuracy:0.877300\n",
      "Iter:2, Test accuracy:0.976400\n",
      "Iter:3, Test accuracy:0.981400\n",
      "Iter:4, Test accuracy:0.983000\n",
      "Iter:5, Test accuracy:0.983700\n",
      "Iter:6, Test accuracy:0.987100\n",
      "Iter:7, Test accuracy:0.988600\n",
      "Iter:8, Test accuracy:0.987900\n",
      "Iter:9, Test accuracy:0.989000\n",
      "Iter:10, Test accuracy:0.988900\n",
      "Iter:11, Test accuracy:0.989000\n",
      "Iter:12, Test accuracy:0.989900\n",
      "Iter:13, Test accuracy:0.990100\n",
      "Iter:14, Test accuracy:0.991700\n",
      "Iter:15, Test accuracy:0.992100\n",
      "Iter:16, Test accuracy:0.990400\n",
      "Iter:17, Test accuracy:0.991800\n",
      "Iter:18, Test accuracy:0.991600\n",
      "Iter:19, Test accuracy:0.991900\n",
      "Iter:20, Test accuracy:0.991000\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "#初始化权值\n",
    "def weight_variable(shape):\n",
    "    init = tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "#初始化偏执单元对应权值\n",
    "def bias_variable(shape):\n",
    "    init = tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "#实现卷积层\n",
    "def conv2d(x,W):\n",
    "    #input : 输入的要做卷积的图片，要求为一个张量，shape为 [ batch, in_height, in_weight, in_channel ]，其中batch为图片的数量，in_height 为图片高度，in_weight 为图片宽度，in_channel 为图片的通道数，灰度图该值为1，彩色图为3。（也可以用其它值，但是具体含义不是很理解）\n",
    "    #filter： 卷积核，要求也是一个张量，shape为 [ filter_height, filter_weight, in_channel, out_channels ]，其中 filter_height 为卷积核高度，filter_weight 为卷积核宽度，in_channel 是图像通道数 ，和 input 的 in_channel 要保持一致，out_channel 是卷积核数量。\n",
    "    #strides： 卷积时在图像每一维的步长，这是一个一维的向量，[ 1, strides, strides, 1]，第一位和最后一位固定必须是1\n",
    "    #padding： string类型，值为“SAME” 和 “VALID”，表示的是卷积的形式，是否考虑边界。\"SAME\"是考虑边界，不足的时候用0去填充周围，\"VALID\"则不考虑\n",
    "    #use_cudnn_on_gpu： bool类型，是否使用cudnn加速，默认为true\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding=\"SAME\")\n",
    "\n",
    "#实现池化层\n",
    "def max_pool_2x2(x):\n",
    "    #value : 需要池化的输入，一般池化层接在卷积层后面，所以输入通常是feature map，依然是[batch_size, height, width, channels]这样的shape\n",
    "    #k_size : 池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]，因为我们不想在batch和channels上做池化，所以这两个维度设为了1\n",
    "    #strides : 窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]\n",
    "    #padding： 填充的方法，SAME或VALID\n",
    "    return tf.nn.max_pool(x,[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "#定义两个输入占位符号\n",
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "#改变x的格式转化为4D的响亮[batch,in_height,in_width,in_channels]\n",
    "x_image = tf.reshape(x,[-1,28,28,1]) #batch：-1 的应用:-1 表示不知道该填什么数字合适的情况下，可以选择，由python通过a和其他的值3推测出来\n",
    "\n",
    "#初始化的一个卷基层的权值和偏执\n",
    "W_conv1 = weight_variable([5,5,1,32]) #5*5的采样窗口，1表示输入通道为1,黑白。32表示输出的通道数--表示使用了32个卷积核（最终会获得32个特征平面）\n",
    "b_conv1 = bias_variable([32]) #每一个卷积核都需要一个偏执，所以我们这里要32个偏置值\n",
    "\n",
    "#获取第一层卷积之后的激活值，以及池化层处理以后的隐藏层信息\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "#初始化第二个卷积层的权值和偏置\n",
    "W_conv2 = weight_variable([5,5,32,64]) #对比上一个的卷积层权值初始化，这里采用5*5采样，32表示输入的维度，64表示使用的卷积核数量（输出的维度）\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "#二次卷积、池化操作\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "#总结上面两次卷积池化操作\n",
    "#第一次：卷积后28×28不变，池化后14×14\n",
    "#第二次：卷积后14×14不变，池化后7×7\n",
    "#获取了64个7×7二维数据(7*7*64个神经单元)\n",
    "\n",
    "#先将第二次池化层处理后的数据扁平化\n",
    "h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])\n",
    "\n",
    "#初始化第一个全链接层权值\n",
    "W_fc1 = weight_variable([7*7*64,1024]) #设置第一个全链接层单元数量为1024\n",
    "b_fc1 = bias_variable([1024]) #共有1024个目标单元\n",
    "\n",
    "#求得第一个全链接层的输出\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1)\n",
    "\n",
    "#设置dropout信息\n",
    "keep_drop = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1,keep_drop)\n",
    "\n",
    "#初始化第二个全链接层权值\n",
    "W_fc2 = weight_variable([1024,10]) #开始要进行输出了\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)\n",
    "\n",
    "#交叉熵处理获取损失\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n",
    "\n",
    "#根据损失函数进行优化\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "#返回预测结果到bool列表\n",
    "corrent_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "\n",
    "#返回准确率tf.cast强制转换\n",
    "accuracy = tf.reduce_mean(tf.cast(corrent_prediction,tf.float32)) \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for iter in range(21):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_drop:0.7})\n",
    "\n",
    "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_drop:1.0})\n",
    "        print(\"Iter:%d, Test accuracy:%f\"%(iter,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
    "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
    "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
    "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
    "Iter:0, Test accuracy:0.865700\n",
    "Iter:1, Test accuracy:0.877300\n",
    "Iter:2, Test accuracy:0.976400\n",
    "Iter:3, Test accuracy:0.981400\n",
    "Iter:4, Test accuracy:0.983000\n",
    "Iter:5, Test accuracy:0.983700\n",
    "Iter:6, Test accuracy:0.987100\n",
    "Iter:7, Test accuracy:0.988600\n",
    "Iter:8, Test accuracy:0.987900\n",
    "Iter:9, Test accuracy:0.989000\n",
    "Iter:10, Test accuracy:0.988900\n",
    "Iter:11, Test accuracy:0.989000\n",
    "Iter:12, Test accuracy:0.989900\n",
    "Iter:13, Test accuracy:0.990100\n",
    "Iter:14, Test accuracy:0.991700\n",
    "Iter:15, Test accuracy:0.992100\n",
    "Iter:16, Test accuracy:0.990400\n",
    "Iter:17, Test accuracy:0.991800\n",
    "Iter:18, Test accuracy:0.991600\n",
    "Iter:19, Test accuracy:0.991900\n",
    "Iter:20, Test accuracy:0.991000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
